# Benchmark Scenarios Directory

This directory contains benchmark scenario definitions and results.

## Files

### Scenario Definitions
Generated by `generate_benchmark_scenarios.py`:
- `minimal.json` - 2 scenarios, smoke test (~1 minute)
- `fast.json` - 14 scenarios, validation suite (~10-15 minutes)
- `quick.json` - 10 scenarios, rapid iteration (~5-10 minutes)
- `full.json` - 60 scenarios, comprehensive analysis (~2-4 hours)

### Results Files
Results are saved to `../results/` with timestamps:
- `../results/minimal_20231215_143022.json` - Minimal suite results
- `../results/fast_20231215_150000.json` - Fast suite results
- `../results/quick_20231215_160000.json` - Quick suite results
- `../results/full_20231215_180000.json` - Full suite results

## Usage

### Generate Scenarios
```bash
# Generate specific suite
python demo/v2/generate_benchmark_scenarios.py --suite fast

# All suites are saved to this directory by default
```

### Run Benchmarks
```bash
# Run benchmark suite
uv run demo/v2/run_benchmark_scenarios.py demo/v2/scenarios/fast.json

# Results saved to demo/v2/results/fast_<timestamp>.json automatically
```

## Git Tracking

- Scenario definitions in this directory are tracked in git
- Results in `../results/` are ignored (timestamped, kept local)
- Share scenario definitions, keep results local
